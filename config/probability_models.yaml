# probability_models.yaml - Statistical Models & Edge Detection
# ============================================
# This file controls how we calculate win probabilities and detect edges.
# Think of this as the "brain" of the system - it determines what's a
# good bet and what's not.
#
# The fundamental equation:
#   Edge = (True Probability × Payout) - 1
#
# Example: Market says Team A 50% chance ($0.50 price).
#          Our model says 60% chance.
#          Edge = (0.60 × 2.0) - 1 = 0.20 = 20% edge
#
# Version: 2.0 (Phase 0.5 - Added Versioning Support)
# Last Updated: 2025-10-21
# ============================================

# ============================================
# MODEL VERSIONING & LIFECYCLE (Phase 0.5)
# ============================================
# CRITICAL CONCEPT: Immutable model versions for A/B testing
# WHY: Cannot compare model v1.0 vs v1.1 if v1.0 keeps changing!
#      Immutable versions allow rigorous comparison and attribution
#
# VERSIONING PHILOSOPHY:
# - Each model configuration gets unique version (e.g., "elo_nfl_v1.0")
# - Once deployed, version is IMMUTABLE (never changes)
# - To modify: create NEW version (e.g., "elo_nfl_v1.1")
# - Old versions stay in database for historical analysis
#
# SEMANTIC VERSIONING:
# - v1.0 → v1.1: Minor update (bug fix, small parameter change)
#   Example: Change k_factor from 32 to 35
# - v1.0 → v2.0: Major update (significant algorithm change)
#   Example: Switch from simple Elo to margin-adjusted Elo
#
# LIFECYCLE STATES:
# 1. DRAFT: Being developed, not deployed
# 2. TESTING: In paper trading or limited deployment
# 3. ACTIVE: Fully deployed, generating real trades
# 4. DEPRECATED: Replaced by newer version, kept for records
#
# EXAMPLE LIFECYCLE:
# elo_nfl_v1.0 (ACTIVE)   → Deployed, making predictions
#     ↓ (found k_factor too low)
# elo_nfl_v1.1 (TESTING)  → Testing in parallel with v1.0
#     ↓ (v1.1 performs 2% better over 30 days)
# elo_nfl_v1.0 (DEPRECATED) + elo_nfl_v1.1 (ACTIVE)
#
# DATABASE SCHEMA (probability_models table):
# - model_id: Primary key
# - model_name: "elo_nfl"
# - model_version: "v1.0"
# - status: "draft" | "testing" | "active" | "deprecated"
# - config: JSONB (full configuration from this file)
# - created_at: Timestamp
#
# USAGE IN TRADING:
# - Each trade records model_id used
# - Enables attribution: Which model generated profitable trades?
# - Enables A/B testing: Compare v1.0 vs v1.1 performance side-by-side

versioning:
  # Enable model versioning?
  # WHY: Required for Phase 4+ when multiple models compete
  enabled: true

  # Version naming convention
  # Format: {model_type}_{sport}_{version}
  # Examples:
  #   - elo_nfl_v1.0
  #   - regression_nba_v2.3
  #   - ensemble_mlb_v1.0
  naming_convention: "{model_type}_{sport}_v{major}.{minor}"

  # Version lifecycle management
  lifecycle:
    # Require testing period before ACTIVE?
    # WHY: Don't deploy untested models with real money
    require_testing: true
    min_testing_days: 30  # Minimum 30 days in TESTING
    min_testing_predictions: 100  # Minimum 100 predictions

    # Auto-deprecate old versions?
    # WHY: Once v1.1 is ACTIVE, v1.0 should be DEPRECATED
    auto_deprecate_on_replacement: true

    # Keep deprecated versions in database?
    # WHY: Historical analysis and audit trail
    retain_deprecated: true
    retention_years: 5  # Keep for 5 years

  # A/B Testing configuration
  # WHY: Run multiple model versions side-by-side to compare
  ab_testing:
    enabled: true

    # How to split traffic between versions?
    # Example: v1.0 (80%) vs v1.1 (20%)
    # WHY: Don't risk all capital on unproven v1.1
    default_traffic_split:
      existing_version: 0.80  # 80% traffic
      new_version: 0.20       # 20% traffic

    # Evaluation criteria for promotion
    # WHY: Need objective criteria to promote v1.1 → ACTIVE
    promotion_criteria:
      min_improvement_pct: 0.02  # 2% better edge accuracy
      min_evaluation_period_days: 30
      confidence_level: 0.95  # 95% statistical significance

# ============================================

# ============================================
# MODEL ARCHITECTURE
# ============================================
# Which modeling approach to use?

architecture:
  # Primary model type
  # Options:
  # - 'elo': Elo rating system (simple, transparent)
  # - 'regression': Logistic regression (moderate complexity)
  # - 'ensemble': Multiple models combined (best accuracy)
  # - 'ml': Machine learning (neural nets, random forest)
  #
  # RECOMMENDATION: Start with 'elo', add complexity later
  primary_model: elo
  
  # Use multiple models and combine predictions?
  # WHY: Different models catch different patterns.
  # Ensemble often more accurate than any single model.
  ensemble:
    enabled: false  # Phase 4+
    models:
      - elo
      - regression
      - ml
    weights:  # How much to trust each model
      elo: 0.40
      regression: 0.35
      ml: 0.25

# ============================================
# ELO RATING SYSTEM
# ============================================
# Classic chess-style rating system adapted for sports.
#
# HOW IT WORKS:
# 1. Each team starts with 1500 rating
# 2. After each game, winner gains points, loser loses points
# 3. Amount transferred depends on rating difference and margin
# 4. Higher rating = higher win probability
#
# WHY ELO:
# - Simple to understand and debug
# - Self-correcting (adapts to team changes)
# - Proven track record (used in chess, sports, games)
# - Transparent (no black box)

elo:
  # Initial rating for new teams
  # WHY: 1500 is standard, represents "average" team
  initial_rating: 1500
  
  # K-factor (how much ratings change per game)
  # Higher K = faster adaptation but more volatility
  # Lower K = slower adaptation but more stability
  #
  # RECOMMENDATION: Vary by sport based on variance
  k_factor:
    nfl: 32   # Lower (season is short, each game important)
    nba: 24   # Medium (long season, more stability)
    mlb: 16   # Lower (very long season, lots of games)
    nhl: 24
    soccer: 28
    tennis: 40  # Higher (individual players, more variance)
  
  # Margin of victory adjustment
  # WHY: Winning by 30 tells you more than winning by 3
  # Multiply K-factor by MOV multiplier
  mov_adjustment:
    enabled: true
    # Formula: multiplier = ln(MOV + 1) / 2
    # Capped at max_multiplier to prevent over-weighting blowouts
    max_multiplier: 2.0
  
  # Home field advantage
  # WHY: Home teams win more often across all sports
  # Add this to home team's rating for probability calc
  home_advantage:
    nfl: 65   # ~3 points in spread terms
    nba: 80   # ~4 points
    mlb: 40   # ~2 points
    nhl: 60
    soccer: 50
  
  # Recency weighting
  # WHY: Recent games matter more than games from 2 years ago
  # Apply exponential decay to old games
  recency_decay:
    enabled: true
    # Half-life in days (games this many days ago worth 50% weight)
    half_life_days: 180  # 6 months
  
  # Regression to mean
  # WHY: Extreme ratings tend to revert toward average over time
  # Pull ratings toward mean during off-season
  regression:
    enabled: true
    # How much to regress (0 = none, 1 = full regression to mean)
    factor: 0.20  # 20% regression
    # When to apply (typically off-season)
    apply_frequency: "off_season"
  
  # Convert Elo rating difference to win probability
  # FORMULA: P(win) = 1 / (1 + 10^((rating_diff / scaling_factor)))
  # WHY: Logistic curve - small differences = close games,
  #      large differences = lopsided
  probability_scaling:
    # Scaling factor (higher = flatter curve)
    # 400 is chess standard, but sports vary
    nfl: 400
    nba: 350
    mlb: 400
    nhl: 380
    tennis: 450

# ============================================
# REGRESSION MODEL
# ============================================
# Statistical regression using game/team features.
#
# HOW IT WORKS:
# 1. Collect features (team stats, player stats, matchup info)
# 2. Train logistic regression: P(win) = logistic(β₀ + β₁×feature₁ + ...)
# 3. Use trained model to predict new games

regression:
  enabled: false  # Phase 3+
  
  # Which features to include?
  # WHY: More features = more accuracy, but also more complexity
  features:
    team_stats:
      - "offensive_efficiency"  # Points per possession
      - "defensive_efficiency"
      - "turnover_rate"
      - "rebounding_rate"  # NBA
      - "yards_per_play"  # NFL
      - "success_rate"  # NFL
    
    matchup_stats:
      - "pace_differential"  # Fast vs slow teams
      - "style_mismatch"  # Run-heavy vs run defense
      - "rest_differential"  # Days since last game
    
    situational:
      - "home_away"
      - "day_of_week"
      - "weather"  # For outdoor sports
      - "injury_impact"
  
  # Regularization
  # WHY: Prevents overfitting to training data
  regularization:
    type: "l2"  # Ridge regression
    alpha: 0.01
  
  # Training parameters
  training:
    # How much historical data to use?
    lookback_years: 5
    
    # Re-train frequency
    # WHY: Teams change, must update model
    retrain_frequency: "weekly"
    
    # Validation method
    validation: "time_series_split"  # Don't peek into future
    validation_split: 0.20  # 20% for validation

# ============================================
# MACHINE LEARNING MODELS
# ============================================
# Advanced ML: neural networks, random forests, gradient boosting.

ml:
  enabled: false  # Phase 7+
  
  # Which ML algorithm?
  # Options:
  # - 'random_forest': Ensemble of decision trees
  # - 'gradient_boosting': XGBoost, LightGBM
  # - 'neural_network': Deep learning
  algorithm: gradient_boosting
  
  # Feature engineering
  feature_engineering:
    # Create interaction features (e.g., offense_rank × defense_rank)
    interactions: true
    
    # Create polynomial features (e.g., stat²)
    polynomials: false
    
    # Use time-series features (rolling averages, trends)
    time_series: true
  
  # Hyperparameters (for gradient boosting)
  hyperparameters:
    n_estimators: 100
    max_depth: 6
    learning_rate: 0.05
    subsample: 0.80
  
  # Model selection
  # WHY: Try multiple hyperparameters, pick best
  hyperparameter_tuning:
    enabled: true
    method: "grid_search"  # Try all combinations
    cv_folds: 5  # 5-fold cross-validation

# ============================================
# EDGE DETECTION
# ============================================
# How to calculate edge from model probabilities.

edge_detection:
  # Edge calculation method
  # Options:
  # - 'simple': Edge = (TrueProb × Payout) - 1
  # - 'kelly': Calculate optimal Kelly bet size
  # - 'expected_value': E[return] accounting for all outcomes
  method: simple
  
  # Adjustment for model uncertainty
  # WHY: Model is not perfect. Discount predictions.
  uncertainty_discount:
    enabled: true
    # Shrink probabilities toward 50% (maximum uncertainty)
    # Example: Model says 65%, discount to 62%
    shrinkage_factor: 0.10  # 10% shrinkage
  
  # Transaction cost adjustment
  # WHY: Kalshi charges fees, reduce effective edge
  transaction_costs:
    enabled: true
    # Kalshi: Maker rebate (get paid for providing liquidity)
    maker_rebate: 0.01  # 1% rebate
    # Kalshi: Taker fee (pay for taking liquidity)
    taker_fee: 0.01  # 1% fee
    # Conservative: assume we're taker (pay fee)
    assume_taker: true
  
  # Slippage adjustment
  # WHY: Price moves between detection and execution
  slippage:
    enabled: true
    # Assume this much average slippage
    expected_slippage_pct: 0.005  # 0.5%
  
  # Minimum edge thresholds (by strategy)
  # WHY: Different strategies have different requirements
  min_edge:
    pre_game: 0.08  # 8% minimum (more uncertainty)
    halftime: 0.06  # 6% minimum (have actual game data)
    live: 0.10  # 10% minimum (faster moving, riskier)
    settlement: 0.02  # 2% minimum (near-certain outcomes)

# ============================================
# PROBABILITY CALIBRATION
# ============================================
# Ensure model probabilities are well-calibrated.
#
# WHAT IS CALIBRATION?
# If model says 60% win probability across 100 games,
# we should win ~60 of them. If we win 70, model is underconfident.
# If we win 50, model is overconfident.

calibration:
  # Calibrate probabilities?
  # WHY: Raw model outputs often poorly calibrated
  enabled: true
  
  # Calibration method
  # Options:
  # - 'platt': Logistic regression on model outputs
  # - 'isotonic': Non-parametric (more flexible)
  method: platt
  
  # Training for calibration
  # WHY: Need separate validation set to avoid overfitting
  calibration_set_size: 0.15  # 15% of data
  
  # Calibration diagnostics
  # Track calibration quality over time
  diagnostics:
    enabled: true
    # Bin predictions and compare to actual outcomes
    bins: 10  # Divide [0%, 100%] into 10 bins
    # Alert if calibration error too high
    max_calibration_error: 0.05  # 5%

# ============================================
# MARKET EFFICIENCY ADJUSTMENT
# ============================================
# Account for market efficiency (how smart is the market?)
#
# WHY: In efficient markets, prices are accurate.
# Our model must be significantly better to find edges.
# In inefficient markets, more opportunities exist.

market_efficiency:
  # Estimate market efficiency by sport/market
  # Scale: 0 = completely random, 1 = perfectly efficient
  estimated_efficiency:
    nfl:
      game_winner: 0.75  # Pretty efficient (Vegas lines)
      player_props: 0.60  # Less efficient
      live_trading: 0.55  # Even less efficient (fast-moving)
    
    nba:
      game_winner: 0.80  # Very efficient
      player_props: 0.65
      live_trading: 0.60
    
    mlb:
      game_winner: 0.70
    
    # Lower efficiency = easier to find edges
  
  # Adjust required edge based on efficiency
  # WHY: In efficient markets, need bigger edge to be confident
  efficiency_adjustment:
    enabled: true
    # For each 0.10 increase in efficiency, require 1% more edge
    edge_increase_per_0.1_efficiency: 0.01

# ============================================
# SITUATIONAL ADJUSTMENTS
# ============================================
# Adjust probabilities for game situations.
#
# EXAMPLE: Model says Team A 60% to win. But it's a rivalry game,
# backup QB starting, terrible weather. Adjust accordingly.

situational_adjustments:
  # Weather adjustments (outdoor sports)
  weather:
    enabled: true
    factors:
      # Wind affects passing (NFL, MLB)
      wind:
        threshold_mph: 15
        passing_adjustment: -0.05  # Reduce passing team prob by 5%
      
      # Rain affects ball handling
      rain:
        threshold_inches: 0.25
        turnover_increase: 0.15  # 15% more turnovers
      
      # Snow is chaos
      snow:
        # Discount model confidence by 20%
        confidence_discount: 0.20
      
      # Temperature extremes
      temperature:
        cold_threshold_f: 20
        cold_adjustment: -0.03  # Slight disadvantage
        hot_threshold_f: 95
        hot_adjustment: -0.02
  
  # Rest/schedule adjustments
  rest:
    enabled: true
    # Back-to-back games (NBA)
    back_to_back:
      disadvantage: 0.03  # 3% probability decrease
    
    # Rest advantage (NFL)
    extra_rest_days:
      advantage_per_day: 0.01  # 1% per extra day (max 3 days)
    
    # Travel distance (all sports)
    travel:
      enabled: true
      # Long cross-country flight
      coast_to_coast: 0.02  # 2% disadvantage
      # Time zone changes
      timezone_change_disadvantage: 0.01  # per hour
  
  # Injury adjustments
  injuries:
    enabled: true
    # Automatic adjustment based on player importance
    # WHY: Losing your star QB is different than backup TE
    key_player_impact:
      quarterback: 0.10  # 10% probability swing (NFL)
      star_player: 0.08  # 8% (NBA, other sports)
      starter: 0.03  # 3%
      bench: 0.01  # 1%
  
  # Motivation adjustments
  # WHY: Playoff races, rivalry games, etc. affect performance
  motivation:
    enabled: true
    factors:
      playoff_implications: 0.03  # 3% boost if fighting for playoffs
      rivalry_game: 0.02  # 2% boost
      revenge_game: 0.02  # 2% boost (lost to this team last time)
      tanking: -0.05  # 5% decrease if team tanking for draft picks

# ============================================
# LIVE GAME ADJUSTMENTS
# ============================================
# How to update probabilities during live games.

live_updates:
  enabled: false  # Phase 3+
  
  # Update frequency
  # WHY: Balance between accuracy and API costs
  update_frequency_seconds: 30
  
  # What events trigger re-calculation?
  trigger_events:
    - "score"
    - "turnover"
    - "injury"
    - "timeout"
    - "quarter_end"
    - "two_minute_warning"
  
  # Live model type
  # Options:
  # - 'win_probability': Real-time win prob model
  # - 'score_differential': Based on expected scoring
  # - 'possession': Based on who has ball, field position
  model_type: win_probability
  
  # Factors in live model
  factors:
    score_differential: 0.35
    time_remaining: 0.25
    field_position: 0.15  # NFL
    possession: 0.15
    team_strength: 0.10  # Elo rating

# ============================================
# BACKTESTING & VALIDATION
# ============================================
# Test models on historical data.

backtesting:
  # Date range for backtesting
  train_start: "2019-09-01"
  train_end: "2023-12-31"
  test_start: "2024-01-01"
  test_end: "2024-12-31"
  
  # Walk-forward validation
  # WHY: Mimics real trading (no peeking into future)
  walk_forward:
    enabled: true
    training_window_months: 12  # Train on 12 months
    test_window_months: 3  # Test on next 3 months
    step_size_months: 1  # Step forward 1 month each time
  
  # Metrics to track
  metrics:
    - "log_loss"  # Calibration quality
    - "brier_score"  # Probability accuracy
    - "auc_roc"  # Discrimination
    - "accuracy"  # % of correct predictions
    - "edge_accuracy"  # Do predicted edges translate to profit?
  
  # Benchmarks
  # WHY: Compare to naive strategies
  benchmarks:
    - "market_probability"  # Just use market price
    - "random"  # Coin flip
    - "historical_average"  # Use team's win rate

# ============================================
# MODEL MONITORING
# ============================================
# Track model performance in production.

monitoring:
  # Track actual vs predicted outcomes
  enabled: true
  
  # Metrics to monitor
  metrics:
    # Calibration: Are 60% predictions winning 60%?
    calibration_error: 
      alert_threshold: 0.08  # 8% error
    
    # Profit: Are edges translating to profit?
    edge_profitability:
      alert_threshold: 0.50  # If edges only 50% profitable, issue
    
    # Accuracy: Are we predicting winners correctly?
    accuracy:
      alert_threshold: 0.52  # Should beat 50% by some margin
  
  # Alert if model degrading
  degradation_alerts:
    enabled: true
    # Compare recent performance to historical
    lookback_window_days: 30
    min_performance_pct: 0.85  # Alert if < 85% of historical
  
  # Automatic model re-training
  auto_retrain:
    enabled: true
    # Retrain if performance drops
    performance_threshold: 0.90  # 90% of historical
    # Or retrain on schedule
    schedule: "weekly"

# ============================================
# CONFIDENCE INTERVALS
# ============================================
# Quantify model uncertainty.
#
# WHY: Model says 60% win prob, but how confident?
# Maybe it's 60% ± 5% (55-65%) or 60% ± 15% (45-75%).

confidence_intervals:
  enabled: true
  
  # Method to calculate
  # Options:
  # - 'bootstrap': Resample data, recalculate
  # - 'bayesian': Posterior distribution
  # - 'historical': Historical error rates
  method: historical
  
  # Confidence level
  # WHY: 95% means we're 95% confident true value in this range
  confidence_level: 0.95  # 95%
  
  # Use in edge detection?
  # WHY: If confidence interval wide, reduce bet size
  use_in_sizing:
    enabled: true
    # Wider interval = smaller bet
    size_reduction_per_pct_width: 0.02  # 2% per 1% width

# ============================================
# NOTES FOR MODEL DEVELOPMENT
# ============================================
# Model development is iterative. Start simple, add complexity.
#
# PHASE 0-1: Elo rating system only
#   - Simple, transparent
#   - Good baseline
#   - ~52-54% accuracy expected
#
# PHASE 2-3: Add situational adjustments
#   - Weather, rest, injuries
#   - ~54-56% accuracy expected
#
# PHASE 4-5: Add regression model
#   - More features
#   - ~56-58% accuracy expected
#
# PHASE 6-8: Add ML ensemble
#   - Complex features
#   - ~58-60% accuracy expected
#
# REALITY CHECK:
# - Market is ~50% accurate (random)
# - Vegas is ~55% accurate (professional)
# - Top ~1% bettors are ~58-60% accurate
# - Nobody is consistently >65% accurate
#
# So if you achieve 56% accuracy, you're beating most people!
#
# ============================================
# END OF ODDS_MODELS.YAML
# ============================================
