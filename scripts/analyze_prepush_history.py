#!/usr/bin/env python3
"""
Analyze Pre-Push History - Trend Analysis for Local Validation Results.

This script provides insight into pre-push validation patterns over time by
analyzing the JSON summary files generated by run_parallel_checks.py.

Features:
    1. Trend analysis: Track pass/fail rates over time
    2. Flaky test detection: Identify tests that intermittently fail
    3. Duration trends: Spot performance regressions
    4. Phase analysis: See which test phases are most problematic
    5. Branch comparison: Compare validation patterns across branches

Usage:
    # Show summary of last 10 runs
    python scripts/analyze_prepush_history.py

    # Show last 20 runs
    python scripts/analyze_prepush_history.py --limit 20

    # Focus on specific branch
    python scripts/analyze_prepush_history.py --branch main

    # Show flaky test report
    python scripts/analyze_prepush_history.py --flaky

    # Show phase breakdown
    python scripts/analyze_prepush_history.py --phases

    # JSON output for tooling
    python scripts/analyze_prepush_history.py --json

Exit Codes:
    0: Analysis completed successfully
    1: No history files found

Reference:
    - Issue #174: Pre-push history analysis for trend detection
    - run_parallel_checks.py: Generates the JSON summaries analyzed here
    - CI parity: Mirrors GitHub Actions workflow analytics
"""

from __future__ import annotations

import argparse
import json
import sys
from collections import Counter, defaultdict
from pathlib import Path
from typing import Any

# Repository root (script is in scripts/)
REPO_ROOT = Path(__file__).parent.parent
ARTIFACTS_DIR = REPO_ROOT / ".pre-push-artifacts"


def load_history(limit: int = 10, branch: str | None = None) -> list[dict[str, Any]]:
    """
    Load recent pre-push summary files.

    Args:
        limit: Maximum number of files to load (most recent first)
        branch: Optional branch filter

    Returns:
        List of summary dictionaries sorted by timestamp (newest first)

    Educational Note:
        Summary files are named `prepush-summary-YYYYMMDD-HHMMSS.json`.
        We sort by filename (which sorts chronologically) and take the last N.
    """
    if not ARTIFACTS_DIR.exists():
        return []

    # Find all summary files (exclude latest symlink)
    summary_files = [
        f for f in ARTIFACTS_DIR.glob("prepush-summary-*.json") if "latest" not in f.name
    ]

    if not summary_files:
        return []

    # Sort by modification time (newest first) and limit
    summary_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
    summary_files = summary_files[:limit]

    # Load and filter
    history = []
    for f in summary_files:
        try:
            with open(f, encoding="utf-8") as fp:
                data = json.load(fp)
                if branch is None or data.get("branch") == branch:
                    data["_file"] = str(f.name)
                    history.append(data)
        except (json.JSONDecodeError, OSError):
            continue  # Skip malformed files

    return history


def analyze_trends(history: list[dict[str, Any]]) -> dict[str, Any]:
    """
    Analyze trends across pre-push runs.

    Returns dictionary with:
        - pass_rate: Percentage of successful runs
        - avg_duration: Average run duration
        - avg_tests_passed: Average number of tests passed
        - total_runs: Number of runs analyzed
        - latest_result: Most recent run summary
    """
    if not history:
        return {"error": "No history available"}

    total = len(history)
    passed = sum(1 for h in history if h.get("success"))
    durations = [h.get("duration_seconds", 0) for h in history]
    tests_passed = [h.get("tests", {}).get("total_passed", 0) for h in history]

    return {
        "pass_rate": round(passed / total * 100, 1) if total > 0 else 0,
        "total_runs": total,
        "passed_runs": passed,
        "failed_runs": total - passed,
        "avg_duration_seconds": round(sum(durations) / len(durations), 1) if durations else 0,
        "min_duration_seconds": round(min(durations), 1) if durations else 0,
        "max_duration_seconds": round(max(durations), 1) if durations else 0,
        "avg_tests_passed": round(sum(tests_passed) / len(tests_passed), 1) if tests_passed else 0,
        "latest_result": history[0] if history else None,
    }


def detect_flaky_tests(history: list[dict[str, Any]], threshold: int = 2) -> list[dict[str, Any]]:
    """
    Detect tests that fail intermittently (flaky tests).

    A test is considered flaky if it:
    1. Failed in at least `threshold` runs
    2. Also passed in at least one run (i.e., not consistently failing)

    Args:
        history: List of summary dictionaries
        threshold: Minimum failures to be considered flaky

    Returns:
        List of flaky test info with failure counts

    Educational Note:
        Flaky tests are problematic because they erode developer confidence
        in the test suite. A test that fails randomly is worse than no test
        because it trains developers to ignore failures.
    """
    # Count failures per test
    failure_counts: Counter[str] = Counter()

    for run in history:
        failed_in_run = set(run.get("failed_tests", []))

        # Count failures
        for test in failed_in_run:
            failure_counts[test] += 1

    # Identify flaky tests
    flaky = []
    for test, fail_count in failure_counts.items():
        # A test is flaky if it failed multiple times but not every time
        if fail_count >= threshold and fail_count < len(history):
            flaky.append(
                {
                    "test": test,
                    "failures": fail_count,
                    "failure_rate": round(fail_count / len(history) * 100, 1),
                }
            )

    # Sort by failure count descending
    flaky.sort(key=lambda x: x["failures"], reverse=True)
    return flaky


def analyze_phases(history: list[dict[str, Any]]) -> dict[str, dict[str, Any]]:
    """
    Analyze test phase performance over time.

    Returns per-phase statistics:
        - avg_duration: Average phase duration
        - failure_rate: Percentage of runs where phase failed
        - avg_tests: Average number of tests in phase
    """
    phase_stats: dict[str, list[dict[str, Any]]] = defaultdict(list)

    for run in history:
        for phase in run.get("phases", []):
            phase_id = phase.get("phase_id", "unknown")
            phase_stats[phase_id].append(
                {
                    "duration": phase.get("duration_seconds", 0),
                    "passed": phase.get("passed", 0),
                    "failed": phase.get("failed", 0),
                    "errors": phase.get("errors", 0),
                    "skipped": phase.get("skipped", 0),
                    "exit_code": phase.get("exit_code", 0),
                }
            )

    result = {}
    for phase_id, runs in phase_stats.items():
        durations = [r["duration"] for r in runs]
        failures = sum(1 for r in runs if r["exit_code"] != 0)
        tests_per_run = [r["passed"] + r["failed"] + r["errors"] for r in runs]

        result[phase_id] = {
            "total_runs": len(runs),
            "avg_duration_seconds": round(sum(durations) / len(durations), 1) if durations else 0,
            "failure_rate": round(failures / len(runs) * 100, 1) if runs else 0,
            "avg_tests": round(sum(tests_per_run) / len(tests_per_run), 1) if tests_per_run else 0,
        }

    return result


def analyze_branches(history: list[dict[str, Any]]) -> dict[str, dict[str, Any]]:
    """
    Compare validation patterns across branches.

    Returns per-branch statistics:
        - pass_rate: Success rate on this branch
        - avg_duration: Average run duration
        - run_count: Number of runs on this branch
    """
    branch_runs: dict[str, list[dict[str, Any]]] = defaultdict(list)

    for run in history:
        branch = run.get("branch", "unknown")
        branch_runs[branch].append(run)

    result = {}
    for branch, runs in branch_runs.items():
        passed = sum(1 for r in runs if r.get("success"))
        durations = [r.get("duration_seconds", 0) for r in runs]

        result[branch] = {
            "run_count": len(runs),
            "pass_rate": round(passed / len(runs) * 100, 1) if runs else 0,
            "avg_duration_seconds": round(sum(durations) / len(durations), 1) if durations else 0,
        }

    return result


def print_summary(history: list[dict[str, Any]]) -> None:
    """Print human-readable summary of pre-push history."""
    if not history:
        print("No pre-push history found in", ARTIFACTS_DIR)
        print("Run a pre-push validation first: python scripts/run_parallel_checks.py")
        return

    trends = analyze_trends(history)

    print("=" * 60)
    print("PRE-PUSH VALIDATION HISTORY")
    print("=" * 60)
    print()

    # Overall trends
    print(f"Total Runs Analyzed: {trends['total_runs']}")
    print(f"Pass Rate: {trends['pass_rate']}% ({trends['passed_runs']}/{trends['total_runs']})")
    print(f"Average Duration: {trends['avg_duration_seconds']}s")
    print(f"Duration Range: {trends['min_duration_seconds']}s - {trends['max_duration_seconds']}s")
    print(f"Average Tests Passed: {trends['avg_tests_passed']}")
    print()

    # Latest run
    latest = trends.get("latest_result")
    if latest:
        status = "PASSED" if latest.get("success") else "FAILED"
        timestamp = latest.get("timestamp", "unknown")
        branch = latest.get("branch", "unknown")
        duration = latest.get("duration_seconds", 0)

        print("-" * 60)
        print("LATEST RUN")
        print("-" * 60)
        print(f"Timestamp: {timestamp}")
        print(f"Branch: {branch}")
        print(f"Status: {status}")
        print(f"Duration: {duration}s")

        tests = latest.get("tests", {})
        print(
            f"Tests: {tests.get('total_passed', 0)} passed, {tests.get('total_failed', 0)} failed, {tests.get('total_errors', 0)} errors"
        )

        failed = latest.get("failed_tests", [])
        if failed:
            print(f"Failed Tests ({len(failed)}):")
            for test in failed[:10]:  # Show first 10
                print(f"  - {test}")
            if len(failed) > 10:
                print(f"  ... and {len(failed) - 10} more")
    print()


def print_flaky_report(history: list[dict[str, Any]]) -> None:
    """Print flaky test analysis."""
    if not history:
        print("No history available for flaky test analysis.")
        return

    flaky = detect_flaky_tests(history)

    print("=" * 60)
    print("FLAKY TEST REPORT")
    print("=" * 60)
    print()

    if not flaky:
        print("No flaky tests detected!")
        print("(A test is flaky if it failed 2+ times but not every time)")
    else:
        print(f"Found {len(flaky)} potentially flaky test(s):")
        print()
        for item in flaky:
            print(f"  {item['test']}")
            print(f"    Failures: {item['failures']} ({item['failure_rate']}% failure rate)")
        print()
        print("Consider investigating these tests for non-deterministic behavior.")
    print()


def print_phase_report(history: list[dict[str, Any]]) -> None:
    """Print test phase analysis."""
    if not history:
        print("No history available for phase analysis.")
        return

    phases = analyze_phases(history)

    print("=" * 60)
    print("TEST PHASE ANALYSIS")
    print("=" * 60)
    print()

    if not phases:
        print("No phase data available.")
    else:
        for phase_id, stats in sorted(phases.items()):
            print(f"[{phase_id}]")
            print(f"  Runs: {stats['total_runs']}")
            print(f"  Avg Duration: {stats['avg_duration_seconds']}s")
            print(f"  Failure Rate: {stats['failure_rate']}%")
            print(f"  Avg Tests: {stats['avg_tests']}")
            print()
    print()


def print_branch_report(history: list[dict[str, Any]]) -> None:
    """Print branch comparison."""
    if not history:
        print("No history available for branch analysis.")
        return

    branches = analyze_branches(history)

    print("=" * 60)
    print("BRANCH COMPARISON")
    print("=" * 60)
    print()

    if not branches:
        print("No branch data available.")
    else:
        for branch, stats in sorted(
            branches.items(), key=lambda x: x[1]["run_count"], reverse=True
        ):
            print(f"[{branch}]")
            print(f"  Runs: {stats['run_count']}")
            print(f"  Pass Rate: {stats['pass_rate']}%")
            print(f"  Avg Duration: {stats['avg_duration_seconds']}s")
            print()
    print()


def main() -> int:
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Analyze pre-push validation history for trends and patterns"
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=10,
        help="Number of recent runs to analyze (default: 10)",
    )
    parser.add_argument(
        "--branch",
        type=str,
        default=None,
        help="Filter to specific branch",
    )
    parser.add_argument(
        "--flaky",
        action="store_true",
        help="Show flaky test report",
    )
    parser.add_argument(
        "--phases",
        action="store_true",
        help="Show phase breakdown",
    )
    parser.add_argument(
        "--branches",
        action="store_true",
        help="Show branch comparison",
    )
    parser.add_argument(
        "--json",
        action="store_true",
        dest="output_json",
        help="Output as JSON for tooling",
    )

    args = parser.parse_args()

    # Load history
    history = load_history(limit=args.limit, branch=args.branch)

    if not history:
        if args.output_json:
            print(
                json.dumps({"error": "No history available", "artifacts_dir": str(ARTIFACTS_DIR)})
            )
        else:
            print(f"No pre-push history found in {ARTIFACTS_DIR}")
            print("Run a pre-push validation first: python scripts/run_parallel_checks.py")
        return 1

    if args.output_json:
        # Build complete analysis for JSON output
        output = {
            "artifacts_dir": str(ARTIFACTS_DIR),
            "trends": analyze_trends(history),
            "flaky_tests": detect_flaky_tests(history),
            "phases": analyze_phases(history),
            "branches": analyze_branches(history),
            "history": history,
        }
        print(json.dumps(output, indent=2, default=str))
    else:
        # Human-readable output
        print_summary(history)

        if args.flaky:
            print_flaky_report(history)

        if args.phases:
            print_phase_report(history)

        if args.branches:
            print_branch_report(history)

    return 0


if __name__ == "__main__":
    sys.exit(main())
